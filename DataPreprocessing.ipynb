{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132fa9f7",
   "metadata": {},
   "source": [
    "# Create a dataset for efficient gpt (pre-)training.\n",
    "- Load raw text datasets.\n",
    "  - https://huggingface.co/datasets/roneneldan/TinyStories\n",
    "- Make a vocaburary from the train dataset.\n",
    "- Transform token to id.\n",
    "- Make N x K(context_size) array with token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11331a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseong/anaconda3/envs/pytorch_2.3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/minseong/anaconda3/envs/pytorch_2.3/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/minseong/anaconda3/envs/pytorch_2.3/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import ftfy\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import load_dict, save_dict\n",
    "from random import randint\n",
    "from spacy.symbols import ORTH\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1918456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftfy version: 6.2.3\n",
      "spacy version: 3.7.5\n",
      "torchtext version: 0.18.0\n",
      "torch version: 2.3.0\n",
      "datasets version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"ftfy\", \n",
    "        \"spacy\", \n",
    "        \"torchtext\", \n",
    "        \"torch\",\n",
    "        \"datasets\",\n",
    "       ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e9c87",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ebf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    '<unk>',  # default unknown token\n",
    "    '<sot>',  # start of text token\n",
    "    '<eot>',  # end of text token\n",
    "]\n",
    "\n",
    "minimum_text_length = 300 # minimum number of characters in a text\n",
    "vocab_size = 30000\n",
    "min_freq = 2\n",
    "context_size = 128 # context window size for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c967d5",
   "metadata": {},
   "source": [
    "#### Download raw texts from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fa68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('roneneldan/TinyStories', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba0e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datas\n",
      "train: 2,119,719\n",
      "validation: 21,990\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datas\")\n",
    "for k, v in dataset.items():\n",
    "    print(f\"{k}: {len(v):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268663c",
   "metadata": {},
   "source": [
    "#### Check a data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33365cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}\n",
      "{'text': 'Once upon a time, there was an adorable little cat named Kitty. Kitty loved to polish her toy car with a soft cloth. One sunny day, she decided to take her shiny car to the park.\\n\\nAt the park, she met a friendly dog named'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0f339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2119719/2119719 [02:26<00:00, 14453.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 21990/21990 [00:01<00:00, 13871.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess the raw text\n",
    "texts = {}\n",
    "for split, items in dataset.items():\n",
    "    texts[split] = []\n",
    "    for item in tqdm(items):\n",
    "        # remove too short texts.\n",
    "        if len(item['text']) < minimum_text_length:\n",
    "            continue\n",
    "            \n",
    "        # remove newline characters and fix texts with ftfy Lib.\n",
    "        text = ftfy.fix_text(item['text'].replace('\\n', '').lower())\n",
    "        texts[split].append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718887e",
   "metadata": {},
   "source": [
    "#### Load a tokenizer from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dce0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "for token in special_tokens:\n",
    "    nlp.tokenizer.add_special_case(token, [{ORTH: token}])\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67567ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens_from_text_lst(text_lst, tokenizer):\n",
    "    for text in tqdm(text_lst):\n",
    "        yield [token.text for token in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bb319",
   "metadata": {},
   "source": [
    "#### Build a vocab with the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc631969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2117893/2117893 [04:55<00:00, 7158.96it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens_from_text_lst(texts['train'], tokenizer),\n",
    "                                 specials=special_tokens,\n",
    "                                 min_freq=min_freq,\n",
    "                                 max_tokens=vocab_size)\n",
    "vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f570e",
   "metadata": {},
   "source": [
    "#### Check the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebeaa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t.text for t in tokenizer(texts['train'][0].lower())]\n",
    "indices = vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4dd5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<sot>', '<eot>', '.', 'the', 'and', ',', 'to', 'a', 'was']\n",
      "['one', 'day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a']\n",
      "[43, 23, 6, 8, 36, 51, 72, 24, 107, 8]\n",
      "['one', 'day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.lookup_tokens(range(10)))\n",
    "print(tokens[:10])\n",
    "print(indices[:10])\n",
    "print(vocab.lookup_tokens(indices[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4800bf3",
   "metadata": {},
   "source": [
    "#### save the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04c65042",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, './data-store/TinyStories/vocab_size-30000/vocab.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d9797",
   "metadata": {},
   "source": [
    "#### Make Token id list for train and validate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1911e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2117893/2117893 [04:46<00:00, 7389.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 21970/21970 [00:02<00:00, 7768.62it/s]\n"
     ]
    }
   ],
   "source": [
    "token_ids = {}\n",
    "for split, text_lst in texts.items():\n",
    "    token_ids[split] = []\n",
    "    for text in tqdm(text_lst):\n",
    "        tokens = [t.text for t in tokenizer(text)]\n",
    "        ids = vocab.lookup_indices(tokens)\n",
    "        token_ids[split].append(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37fc64",
   "metadata": {},
   "source": [
    "#### Check a token id list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee877830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 50, 8, 37, 6, 35, 9, 8, 36, 159, 72, 1952, 3, 1952, 75, 7, 64, 269, 5, 47, 18, 4, 242, 3, 1952, 9, 8, 891, 159, 178, 10, 137, 27, 138, 2576, 3, 138, 2576, 96, 1952, 40, 5, 5490, 23, 6, 1952, 9, 1629, 18, 4, 102, 78, 10, 42, 8, 41, 147, 3, 4, 147, 27, 182, 524, 21, 45, 2127, 3, 1952, 115, 145, 4, 524, 836, 5, 49, 7, 47, 20, 55, 3, 1952, 687, 403, 4, 147, 5, 300, 4, 524, 836, 28, 53, 3, 10, 209, 5, 5024, 19, 0, 174, 20, 4, 2127, 524, 65, 23, 3, 78, 13, 9, 37, 7, 64, 112, 6, 1952, 153, 10, 458, 144, 2576, 3, 10, 63, 7, 4, 2576, 328, 5, 116, 144, 891, 2576, 3, 190, 6, 1952, 9, 378, 7, 64, 269, 5, 47, 113, 4, 252, 23, 3, 5, 1952, 227, 450, 292, 188, 3]\n",
      "once upon a time , there was a little car named beep . beep loved to go fast and play in the sun . beep was a healthy car because he always had good fuel . good fuel made beep happy and strong.one day , beep was driving in the park when he saw a big tree . the tree had many leaves that were falling . beep liked how the leaves fall and wanted to play with them . beep drove under the tree and watched the leaves fall on him . he laughed and beeped his <unk> played with the falling leaves all day . when it was time to go home , beep knew he needed more fuel . he went to the fuel place and got more healthy fuel . now , beep was ready to go fast and play again the next day . and beep lived happily ever after .\n"
     ]
    }
   ],
   "source": [
    "print(token_ids['train'][1])\n",
    "print(' '.join(vocab.lookup_tokens(token_ids['train'][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361fca1",
   "metadata": {},
   "source": [
    "#### Save a token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71701b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict('./data-store/TinyStories/vocab_size-30000/token_ids.pkl', token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f243fc7",
   "metadata": {},
   "source": [
    "### Make N x K(context size) array for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd94d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7eb71",
   "metadata": {},
   "source": [
    "#### Make a contiguous list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "885748b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 2117893/2117893 [00:03<00:00, 558273.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 21970/21970 [00:00<00:00, 460102.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# make a contiguous list\n",
    "contiguous_ids = {}\n",
    "for split, token_id_lst in token_ids.items():\n",
    "    contiguous_ids[split] = []\n",
    "    for tokens in tqdm(token_id_lst):\n",
    "        # add sot and eot tokens\n",
    "        tokens = [1] + tokens + [2]\n",
    "        \n",
    "        contiguous_ids[split] += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aaf3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 435,441,807\n",
      "validation: 4,385,624\n"
     ]
    }
   ],
   "source": [
    "for k, v in contiguous_ids.items():\n",
    "    print(f\"{k}: {len(v):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ce7f9",
   "metadata": {},
   "source": [
    "#### Reshape a contiguous list to NxK array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e21b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array = {}\n",
    "for split, v in contiguous_ids.items():\n",
    "    arr = np.asarray(v, np.uint16)\n",
    "    length = arr.shape[0]\n",
    "    N = length // context_size\n",
    "    \n",
    "    context_array[split] = arr[:N * context_size].reshape(N, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f556fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3401889, 128)\n",
      "validation: (34262, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in context_array.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1ffa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data-store/TinyStories/vocab_size-30000/train_context_arr.npy', context_array['train'])\n",
    "np.save('./data-store/TinyStories/vocab_size-30000/valid_context_arr.npy', context_array['validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2.3",
   "language": "python",
   "name": "pytorch_2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
